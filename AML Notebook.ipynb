{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kewin\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import libraries for doing image analysis\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "import glob\n",
    "import os\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import StratifiedKFold as KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from pylab import cm\n",
    "from skimage import segmentation\n",
    "from skimage.morphology import watershed\n",
    "from skimage import measure\n",
    "from skimage import morphology\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "from skimage.feature import peak_local_max\n",
    "# make graphics inline\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n",
      "channels_first\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "print(K.image_data_format())\n",
    "K.set_image_data_format('channels_first')\n",
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16118063148174085873\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3282167398\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 1063493276835867971\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# check for GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132103.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66467.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9143.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20630.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33689.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        image  class\n",
       "0  132103.jpg      0\n",
       "1   66467.jpg      0\n",
       "2    9143.jpg      0\n",
       "3   20630.jpg      0\n",
       "4   33689.jpg      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_labels = pd.read_csv(os.path.join(data_dir,'train_onelabel.csv'))\n",
    "df_train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unknown_unclassified</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unknown_sticks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>protist_star</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>copepod_cyclopoid_oithona</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hydromedusae_solmundella</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       label  class\n",
       "0       unknown_unclassified      0\n",
       "1             unknown_sticks      1\n",
       "2               protist_star      2\n",
       "3  copepod_cyclopoid_oithona      3\n",
       "4   hydromedusae_solmundella      4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label_map = pd.read_csv(os.path.join(data_dir,'label_map.txt'), sep=\" \", header=None, names=[\"label\", \"class\"])\n",
    "df_label_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "IMG_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_img(img):\n",
    "    # rescale to standard size\n",
    "    img = resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\train_images\\100088.jpg\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'gray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-269-054d7da2f280>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_grey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'gray'"
     ]
    }
   ],
   "source": [
    "# Example image\n",
    "example_file = glob.glob(os.path.join(data_dir,'train_images/*.jpg'))[12]\n",
    "print(example_file)\n",
    "im = imread(example_file, as_grey=True)\n",
    "plt.imshow(im, cmap=cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_im = preprocess_img(im)\n",
    "plt.imshow(new_im, cmap=cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label(file_name):\n",
    "    return df_train_labels.loc[df_train_labels['image'] == file_name]['class'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 24204\n",
      "Reading images...\n",
      "5.0 % done\n",
      "10.0 % done\n",
      "15.0 % done\n",
      "20.0 % done\n",
      "25.0 % done\n",
      "30.0 % done\n",
      "35.0 % done\n",
      "40.0 % done\n",
      "45.0 % done\n",
      "50.0 % done\n",
      "55.0 % done\n",
      "60.0 % done\n",
      "65.0 % done\n",
      "70.0 % done\n",
      "75.0 % done\n",
      "80.0 % done\n",
      "85.0 % done\n",
      "90.0 % done\n",
      "95.0 % done\n",
      "100.0 % done\n"
     ]
    }
   ],
   "source": [
    "#get the total training images\n",
    "number_of_images = 0\n",
    "for _, _, fileNames in os.walk(os.path.join(data_dir,'train_images')): \n",
    "    for fileName in fileNames:\n",
    "        # Only read in the images\n",
    "        if fileName[-4:] != \".jpg\":\n",
    "            continue\n",
    "        number_of_images += 1\n",
    "        \n",
    "print('Number of images:', number_of_images)\n",
    "\n",
    "imgs = []\n",
    "labels = []\n",
    "i = 0\n",
    "\n",
    "print('Reading images...')\n",
    "\n",
    "for root, _, file_names in os.walk(os.path.join(data_dir,'train_images')): # change in train_images\n",
    "    for file_name in file_names:\n",
    "        # Only read in the images\n",
    "        if fileName[-4:] != \".jpg\":\n",
    "            continue\n",
    "        \n",
    "        img_path = \"{0}{1}{2}\".format(root, os.sep, file_name)\n",
    "        img = preprocess_img(imread(img_path, as_grey=True))\n",
    "        imgs.append(img)\n",
    "        \n",
    "        label = get_label(file_name)\n",
    "        labels.append(label)\n",
    "        \n",
    "        i += 1\n",
    "        # report progress for each 5% done  \n",
    "        report = [int((j+1)*number_of_images/20.) for j in range(20)]\n",
    "        if i in report: print(np.ceil(i *100.0 / number_of_images), \"% done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24204, 64, 64)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(imgs, dtype='float32')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24204, 1, 64, 64)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.reshape(X.shape[0], 1, IMG_SIZE, IMG_SIZE)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24204, 121)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np_utils.to_categorical(labels)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 6132\n",
      "Reading test images...\n",
      "5.0 % done\n",
      "10.0 % done\n",
      "15.0 % done\n",
      "20.0 % done\n",
      "25.0 % done\n",
      "30.0 % done\n",
      "35.0 % done\n",
      "40.0 % done\n",
      "45.0 % done\n",
      "50.0 % done\n",
      "55.0 % done\n",
      "60.0 % done\n",
      "65.0 % done\n",
      "70.0 % done\n",
      "75.0 % done\n",
      "80.0 % done\n",
      "85.0 % done\n",
      "90.0 % done\n",
      "95.0 % done\n",
      "100.0 % done\n"
     ]
    }
   ],
   "source": [
    "#get the total training images\n",
    "number_of_images = 0\n",
    "for _, _, fileNames in os.walk(os.path.join(data_dir,'test_images')): \n",
    "    for fileName in fileNames:\n",
    "        # Only read in the images\n",
    "        if fileName[-4:] != \".jpg\":\n",
    "            continue\n",
    "        number_of_images += 1\n",
    "        \n",
    "print('Number of images:', number_of_images)\n",
    "\n",
    "imgs = []\n",
    "test_img_names = []\n",
    "i = 0\n",
    "\n",
    "print('Reading test images...')\n",
    "\n",
    "for root, _, file_names in os.walk(os.path.join(data_dir,'test_images')): # change in train_images\n",
    "    for file_name in file_names:\n",
    "        # Only read in the images\n",
    "        if fileName[-4:] != \".jpg\":\n",
    "            continue\n",
    "        \n",
    "        img_path = \"{0}{1}{2}\".format(root, os.sep, file_name)\n",
    "        img = preprocess_img(imread(img_path, as_grey=True))\n",
    "        imgs.append(img)\n",
    "        \n",
    "        test_img_names.append(file_name)\n",
    "        \n",
    "        i += 1\n",
    "        # report progress for each 5% done  \n",
    "        report = [int((j+1)*number_of_images/20.) for j in range(20)]\n",
    "        if i in report: print(np.ceil(i *100.0 / number_of_images), \"% done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6132, 64, 64)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array(imgs, dtype='float32')\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6132, 1, 64, 64)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.reshape(X_test.shape[0], 1, IMG_SIZE, IMG_SIZE)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Keras Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unnamed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 121\n",
    "\n",
    "def cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', \n",
    "                     input_shape=(1, IMG_SIZE, IMG_SIZE), \n",
    "                     activation='relu', data_format='channels_first'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', data_format='channels_first'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                     activation='relu', data_format='channels_first'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', data_format='channels_first'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                     activation='relu', data_format='channels_first'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', data_format='channels_first'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19363 samples, validate on 4841 samples\n",
      "Epoch 1/10\n",
      "19363/19363 [==============================] - 122s 6ms/step - loss: 3.4119 - acc: 0.2147 - val_loss: 2.7014 - val_acc: 0.3444\n",
      "Epoch 2/10\n",
      "19363/19363 [==============================] - 120s 6ms/step - loss: 2.4011 - acc: 0.3868 - val_loss: 2.0460 - val_acc: 0.4604\n",
      "Epoch 3/10\n",
      "19363/19363 [==============================] - 120s 6ms/step - loss: 2.0074 - acc: 0.4528 - val_loss: 1.7341 - val_acc: 0.5294\n",
      "Epoch 4/10\n",
      "10592/19363 [===============>..............] - ETA: 50s - loss: 1.8897 - acc: 0.4835"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-281-b05a0922433e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2352\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2353\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, validation_split=0.2, batch_size=32, epochs=10, verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('model.h5')  # always save your weights after training or during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ CategoryAccuracy = \\frac{1}{N} \\sum_{y_i = \\hat{y}_i} 1,$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X, Y, verbose=0)\n",
    "print(\"Accuracy: \", score[1])\n",
    "#.68\n",
    "\n",
    "#0.72\n",
    "\n",
    "#0.8 with img size 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = pd.DataFrame(list(zip(test_img_names, y_pred)),columns=['image','class'])\n",
    "df_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export dataframe to csv file for submission\n",
    "df_submit.to_csv('submission', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cifar10 CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cifar10():\n",
    "    \n",
    "    n_filters = 64\n",
    "    filter_size1 = 3\n",
    "    filter_size2 = 2\n",
    "    pool_size1 = 3\n",
    "    pool_size2 = 1\n",
    "    n_dense = 128\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, filter_size1, filter_size1, \n",
    "                            batch_input_shape=(None, 1, IMG_SIZE, IMG_SIZE), activation='relu', border_mode='valid'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(pool_size1, pool_size1)))\n",
    "\n",
    "    model.add(Conv2D(128, filter_size2, filter_size2, activation='relu', border_mode='valid'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(pool_size2, pool_size2)))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(512))\n",
    "\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(NUM_CLASSES))\n",
    "\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32, (3, 3), padding='same',\n",
    "#                      input_shape=(1, IMG_SIZE, IMG_SIZE), data_format='channels_first'))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Conv2D(32, (3, 3), data_format='channels_first'))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(64, (3, 3), padding='same', data_format='channels_first'))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Conv2D(64, (3, 3), data_format='channels_first'))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "    \n",
    "# #     model.add(Conv2D(128, (3, 3), data_format='channels_first'))\n",
    "# #     model.add(Activation('relu'))\n",
    "# #     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# #     model.add(Dropout(0.25))\n",
    "    \n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(512))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(NUM_CLASSES))\n",
    "#     model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "model_cifar10 = model_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32, (3, 3), padding='same',\n",
    "#                      input_shape=(1, IMG_SIZE, IMG_SIZE), data_format='channels_first'))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Conv2D(32, (3, 3), data_format='channels_first'))\n",
    "#     model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(128, (3, 3), padding='same', data_format='channels_first'))\n",
    "#     model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "#     model.add(Conv2D(128, (3, 3), data_format='channels_first'))\n",
    "#     model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "#     model.add(Conv2D(64, (3, 3), data_format='channels_first'))\n",
    "#     model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "    \n",
    "#     model.add(Conv2D(256, (3, 3), data_format='channels_first'))\n",
    "#     model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "#     model.add(Conv2D(256, (3, 3), data_format='channels_first'))\n",
    "#     model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "#     model.add(Conv2D(128, (3, 3), data_format='channels_first'))\n",
    "#     model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.3))\n",
    "    \n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(512))\n",
    "#     model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(NUM_CLASSES))\n",
    "#     model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "# 0.69 model below\n",
    "\n",
    "# model = Sequential()\n",
    "#     model.add(Conv2D(32, (3, 3), padding='same',\n",
    "#                      input_shape=(1, IMG_SIZE, IMG_SIZE), data_format='channels_first'))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Conv2D(32, (3, 3), data_format='channels_first'))\n",
    "#     model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(64, (3, 3), padding='same', data_format='channels_first'))\n",
    "#     model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "#     model.add(Conv2D(64, (3, 3), data_format='channels_first'))\n",
    "#     model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "#     model.add(Conv2D(128, (3, 3), data_format='channels_first'))\n",
    "#     model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.3))\n",
    "    \n",
    "#     model.add(Conv2D(128, (3, 3), data_format='channels_first'))\n",
    "#     model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "#     model.add(Conv2D(256, (3, 3), data_format='channels_first'))\n",
    "#     model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.3))\n",
    "    \n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(512))\n",
    "#     model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(NUM_CLASSES))\n",
    "#     model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cifar10():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     input_shape=(1, IMG_SIZE, IMG_SIZE), data_format='channels_first'))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(Conv2D(32, (3, 3), data_format='channels_first'))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', data_format='channels_first'))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(Conv2D(64, (3, 3), data_format='channels_first'))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(Conv2D(128, (3, 3), data_format='channels_first'))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), data_format='channels_first'))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(Conv2D(256, (3, 3), data_format='channels_first'))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(NUM_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_cifar10 = model_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model_cifar10.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "\n",
    "# # initiate RMSprop optimizer\n",
    "# opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# # Let's train the model using RMSprop\n",
    "# model_cifar10.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=opt,\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "tb_cifar10 = TensorBoard(log_dir='./logs_cifar10', histogram_freq=0, write_graph=True, write_images=False)\n",
    "\n",
    "esCallback = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=5,\n",
    "                              verbose=0, mode='auto')\n",
    "\n",
    "checkpointCallback = keras.callbacks.ModelCheckpoint('./best_model_checkpoint.hdf5', monitor='val_loss',\n",
    "                                verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cifar10.fit(X, Y, validation_split=0.15, batch_size=64, epochs=3, verbose=1, callbacks=[tb_cifar10, esCallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_cifar10.save_weights('model_cifar12.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_cifar10.evaluate(X, Y, verbose=0)\n",
    "print(\"Accuracy: \", score[1])\n",
    "\n",
    "# 0.8266\n",
    "\n",
    "# 0.86 adding one layer 128\n",
    "\n",
    "# zmienic img size do 90x90 ? wtedy bardziej optymalny przeplyw? albo cos podzielnego przez 3, np 60x60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_cifar10.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_cifar10.predict_classes(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "cr = classification_report(np.argmax(Y_val,axis=1), y_pred, target_names=df_label_map['label'])\n",
    "print(cr)\n",
    "\n",
    "cm = confusion_matrix(np.argmax(Y_val,axis=1), y_pred)\n",
    "print(cm)\n",
    "\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = pd.DataFrame(list(zip(test_img_names, y_pred)),columns=['image','class'])\n",
    "df_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export dataframe to csv file for submission\n",
    "df_submit.to_csv('submission_cifar10_1', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR10 with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb_cifar10_aug = TensorBoard(log_dir='./logs_cifar10_aug', histogram_freq=0, write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "303/303 [==============================] - 94s 310ms/step - loss: 4.1650 - acc: 0.0883 - val_loss: 3.7479 - val_acc: 0.1283\n",
      "Epoch 2/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 3.5643 - acc: 0.1703 - val_loss: 2.9749 - val_acc: 0.2816\n",
      "Epoch 3/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 3.0047 - acc: 0.2682 - val_loss: 2.4833 - val_acc: 0.3675\n",
      "Epoch 4/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 2.7231 - acc: 0.3165 - val_loss: 2.2169 - val_acc: 0.4175\n",
      "Epoch 5/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 2.4152 - acc: 0.3679 - val_loss: 2.0728 - val_acc: 0.4363\n",
      "Epoch 6/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 2.2240 - acc: 0.4035 - val_loss: 1.9167 - val_acc: 0.4724\n",
      "Epoch 7/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 2.0863 - acc: 0.4303 - val_loss: 1.7543 - val_acc: 0.5139\n",
      "Epoch 8/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.9662 - acc: 0.4613 - val_loss: 1.6372 - val_acc: 0.5373\n",
      "Epoch 9/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.8780 - acc: 0.4776 - val_loss: 1.6091 - val_acc: 0.5433\n",
      "Epoch 10/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.7907 - acc: 0.5030 - val_loss: 1.5598 - val_acc: 0.5631\n",
      "Epoch 11/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.7481 - acc: 0.5058 - val_loss: 1.4645 - val_acc: 0.5877\n",
      "Epoch 12/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.7041 - acc: 0.5198 - val_loss: 1.4578 - val_acc: 0.5854\n",
      "Epoch 13/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.6563 - acc: 0.5305 - val_loss: 1.4351 - val_acc: 0.6040\n",
      "Epoch 14/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.6085 - acc: 0.5428 - val_loss: 1.4049 - val_acc: 0.5881\n",
      "Epoch 15/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.5744 - acc: 0.5483 - val_loss: 1.3924 - val_acc: 0.6034\n",
      "Epoch 16/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.5438 - acc: 0.5534 - val_loss: 1.3251 - val_acc: 0.6195\n",
      "Epoch 17/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.5206 - acc: 0.5640 - val_loss: 1.3389 - val_acc: 0.6145\n",
      "Epoch 18/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.4995 - acc: 0.5689 - val_loss: 1.3202 - val_acc: 0.6197\n",
      "Epoch 19/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.4628 - acc: 0.5732 - val_loss: 1.2902 - val_acc: 0.6249\n",
      "Epoch 20/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.4651 - acc: 0.5768 - val_loss: 1.2525 - val_acc: 0.6387\n",
      "Epoch 21/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.4349 - acc: 0.5792 - val_loss: 1.2694 - val_acc: 0.6298\n",
      "Epoch 22/100\n",
      "303/303 [==============================] - 92s 305ms/step - loss: 1.4122 - acc: 0.5886 - val_loss: 1.2344 - val_acc: 0.6397\n",
      "Epoch 23/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.3933 - acc: 0.5923 - val_loss: 1.3285 - val_acc: 0.6147\n",
      "Epoch 24/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.3683 - acc: 0.5951 - val_loss: 1.2417 - val_acc: 0.6472\n",
      "Epoch 25/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.3663 - acc: 0.5947 - val_loss: 1.2414 - val_acc: 0.6389\n",
      "Epoch 26/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.3403 - acc: 0.6041 - val_loss: 1.2492 - val_acc: 0.6478\n",
      "Epoch 27/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.3379 - acc: 0.6022 - val_loss: 1.2089 - val_acc: 0.6505\n",
      "Epoch 28/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.3260 - acc: 0.6081 - val_loss: 1.2340 - val_acc: 0.6362\n",
      "Epoch 29/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.3041 - acc: 0.6139 - val_loss: 1.2114 - val_acc: 0.6478\n",
      "Epoch 30/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.2900 - acc: 0.6159 - val_loss: 1.1767 - val_acc: 0.6616\n",
      "Epoch 31/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.2834 - acc: 0.6149 - val_loss: 1.1794 - val_acc: 0.6600\n",
      "Epoch 32/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.2838 - acc: 0.6152 - val_loss: 1.1505 - val_acc: 0.6637\n",
      "Epoch 33/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.2644 - acc: 0.6214 - val_loss: 1.1846 - val_acc: 0.6618\n",
      "Epoch 34/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.2615 - acc: 0.6235 - val_loss: 1.1476 - val_acc: 0.6606\n",
      "Epoch 35/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.2401 - acc: 0.6309 - val_loss: 1.1884 - val_acc: 0.6596\n",
      "Epoch 36/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.2368 - acc: 0.6294 - val_loss: 1.1587 - val_acc: 0.6587\n",
      "Epoch 37/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.2354 - acc: 0.6266 - val_loss: 1.1247 - val_acc: 0.6722\n",
      "Epoch 38/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.2155 - acc: 0.6353 - val_loss: 1.1297 - val_acc: 0.6711\n",
      "Epoch 39/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.2174 - acc: 0.6325 - val_loss: 1.1573 - val_acc: 0.6658\n",
      "Epoch 40/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.2023 - acc: 0.6371 - val_loss: 1.0874 - val_acc: 0.6775\n",
      "Epoch 41/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.1948 - acc: 0.6373 - val_loss: 1.1051 - val_acc: 0.6782\n",
      "Epoch 42/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.1928 - acc: 0.6402 - val_loss: 1.1372 - val_acc: 0.6722\n",
      "Epoch 43/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.1921 - acc: 0.6386 - val_loss: 1.1071 - val_acc: 0.6722\n",
      "Epoch 44/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.1749 - acc: 0.6437 - val_loss: 1.1034 - val_acc: 0.6685\n",
      "Epoch 45/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.1624 - acc: 0.6430 - val_loss: 1.1382 - val_acc: 0.6693\n",
      "Epoch 46/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.1521 - acc: 0.6501 - val_loss: 1.1497 - val_acc: 0.6678\n",
      "Epoch 47/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.1551 - acc: 0.6472 - val_loss: 1.1031 - val_acc: 0.6765\n",
      "Epoch 48/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.1506 - acc: 0.6507 - val_loss: 1.0771 - val_acc: 0.6802\n",
      "Epoch 49/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.1486 - acc: 0.6502 - val_loss: 1.0995 - val_acc: 0.6784\n",
      "Epoch 50/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.1414 - acc: 0.6525 - val_loss: 1.0673 - val_acc: 0.6790\n",
      "Epoch 51/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.1366 - acc: 0.6558 - val_loss: 1.0764 - val_acc: 0.6815\n",
      "Epoch 52/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.1220 - acc: 0.6581 - val_loss: 1.1109 - val_acc: 0.6773\n",
      "Epoch 53/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.1196 - acc: 0.6599 - val_loss: 1.0788 - val_acc: 0.6885\n",
      "Epoch 54/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.1203 - acc: 0.6536 - val_loss: 1.1267 - val_acc: 0.6809\n",
      "Epoch 55/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.1229 - acc: 0.6513 - val_loss: 1.1506 - val_acc: 0.6736\n",
      "Epoch 56/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.1020 - acc: 0.6603 - val_loss: 1.1068 - val_acc: 0.6720\n",
      "Epoch 57/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0962 - acc: 0.6596 - val_loss: 1.1201 - val_acc: 0.6813\n",
      "Epoch 58/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.1050 - acc: 0.6614 - val_loss: 1.0772 - val_acc: 0.6842\n",
      "Epoch 59/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0995 - acc: 0.6587 - val_loss: 1.0951 - val_acc: 0.6935\n",
      "Epoch 60/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.0976 - acc: 0.6579 - val_loss: 1.0465 - val_acc: 0.6912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0858 - acc: 0.6652 - val_loss: 1.1256 - val_acc: 0.6788\n",
      "Epoch 62/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0813 - acc: 0.6672 - val_loss: 1.0699 - val_acc: 0.6939\n",
      "Epoch 63/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0672 - acc: 0.6706 - val_loss: 1.1602 - val_acc: 0.6616\n",
      "Epoch 64/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.0733 - acc: 0.6686 - val_loss: 1.0350 - val_acc: 0.6930\n",
      "Epoch 65/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0756 - acc: 0.6689 - val_loss: 1.0406 - val_acc: 0.6920\n",
      "Epoch 66/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.0627 - acc: 0.6731 - val_loss: 1.0581 - val_acc: 0.6860\n",
      "Epoch 67/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0692 - acc: 0.6707 - val_loss: 1.0582 - val_acc: 0.6935\n",
      "Epoch 68/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0667 - acc: 0.6706 - val_loss: 1.0472 - val_acc: 0.6860\n",
      "Epoch 69/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.0864 - acc: 0.6678 - val_loss: 1.0813 - val_acc: 0.6866\n",
      "Epoch 70/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0608 - acc: 0.6724 - val_loss: 1.0346 - val_acc: 0.6937\n",
      "Epoch 71/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.0623 - acc: 0.6733 - val_loss: 1.0895 - val_acc: 0.6835\n",
      "Epoch 72/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.0586 - acc: 0.6697 - val_loss: 1.0362 - val_acc: 0.7015\n",
      "Epoch 73/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0517 - acc: 0.6746 - val_loss: 1.0836 - val_acc: 0.6850\n",
      "Epoch 74/100\n",
      "303/303 [==============================] - 96s 317ms/step - loss: 1.0399 - acc: 0.6763 - val_loss: 1.0290 - val_acc: 0.6988\n",
      "Epoch 75/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0294 - acc: 0.6810 - val_loss: 1.0765 - val_acc: 0.6908\n",
      "Epoch 76/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0470 - acc: 0.6750 - val_loss: 1.0877 - val_acc: 0.6769\n",
      "Epoch 77/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.0393 - acc: 0.6751 - val_loss: 1.0910 - val_acc: 0.6866\n",
      "Epoch 78/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 1.0306 - acc: 0.6779 - val_loss: 1.1039 - val_acc: 0.6815\n",
      "Epoch 79/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0291 - acc: 0.6781 - val_loss: 1.0057 - val_acc: 0.7061\n",
      "Epoch 80/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0171 - acc: 0.6845 - val_loss: 0.9897 - val_acc: 0.7108\n",
      "Epoch 81/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0126 - acc: 0.6813 - val_loss: 1.0493 - val_acc: 0.7005\n",
      "Epoch 82/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0300 - acc: 0.6803 - val_loss: 1.0420 - val_acc: 0.6955\n",
      "Epoch 83/100\n",
      "303/303 [==============================] - 93s 308ms/step - loss: 1.0229 - acc: 0.6844 - val_loss: 1.0400 - val_acc: 0.7044\n",
      "Epoch 84/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0206 - acc: 0.6855 - val_loss: 1.0497 - val_acc: 0.6949\n",
      "Epoch 85/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0159 - acc: 0.6863 - val_loss: 1.0581 - val_acc: 0.6963\n",
      "Epoch 86/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0079 - acc: 0.6876 - val_loss: 1.0440 - val_acc: 0.6963\n",
      "Epoch 87/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0070 - acc: 0.6847 - val_loss: 1.0193 - val_acc: 0.7075\n",
      "Epoch 88/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 0.9891 - acc: 0.6898 - val_loss: 1.0219 - val_acc: 0.7098\n",
      "Epoch 89/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 0.9936 - acc: 0.6875 - val_loss: 1.0254 - val_acc: 0.7046\n",
      "Epoch 90/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0059 - acc: 0.6859 - val_loss: 1.0251 - val_acc: 0.7079\n",
      "Epoch 91/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 1.0011 - acc: 0.6866 - val_loss: 1.0216 - val_acc: 0.6976\n",
      "Epoch 92/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 0.9925 - acc: 0.6896 - val_loss: 1.0310 - val_acc: 0.7098\n",
      "Epoch 93/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 0.9975 - acc: 0.6899 - val_loss: 1.0082 - val_acc: 0.7129\n",
      "Epoch 94/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 0.9866 - acc: 0.6885 - val_loss: 1.0086 - val_acc: 0.7061\n",
      "Epoch 95/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 0.9793 - acc: 0.6926 - val_loss: 1.0463 - val_acc: 0.7009\n",
      "Epoch 96/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 0.9774 - acc: 0.6957 - val_loss: 1.0334 - val_acc: 0.7096\n",
      "Epoch 97/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 0.9749 - acc: 0.6951 - val_loss: 1.0806 - val_acc: 0.6877\n",
      "Epoch 98/100\n",
      "303/303 [==============================] - 93s 305ms/step - loss: 0.9919 - acc: 0.6907 - val_loss: 1.1098 - val_acc: 0.6918\n",
      "Epoch 99/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 0.9990 - acc: 0.6856 - val_loss: 1.0692 - val_acc: 0.7096\n",
      "Epoch 100/100\n",
      "303/303 [==============================] - 93s 306ms/step - loss: 0.9723 - acc: 0.6969 - val_loss: 1.0198 - val_acc: 0.7052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23cdaaa3ac8>"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(X_train)\n",
    "\n",
    "steps_per_epoch = int(np.ceil(X_train.shape[0] / float(batch_size)))\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow() ==> Realtime data augmentation\n",
    "model_cifar10.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size), \n",
    "                    epochs=epochs, validation_data=(X_val, Y_val), verbose=1, \n",
    "                            steps_per_epoch=steps_per_epoch, workers=4, callbacks=[tb_cifar10_aug, checkpointCallback])\n",
    "#, esCallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_cifar10.save_weights('model_cifar10_augmentation2_66.h5')\n",
    "\n",
    "model_cifar10 = keras.models.load_model('best_model_checkpoint.hdf5')\n",
    "\n",
    "# POWTORZ TRENOWANIE TEGO MODELU ALE Z EARLY STOP! ^^^^^^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4841/4841 [==============================] - 10s 2ms/step\n",
      "Test loss: 0.989741425282\n",
      "Test accuracy: 0.710803553022\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model_cifar10.evaluate(X_val, Y_val, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "#0.640983267969\n",
    "\n",
    "# 0.6587.. with img size 64 and one conv 128 added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model_cifar10.predict_classes(X_test)\n",
    "df_submit = pd.DataFrame(list(zip(test_img_names, y_pred)),columns=['image','class'])\n",
    "df_submit.to_csv('submission_cifar10_augmentation', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "                             featurewise_std_normalization=False,\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             zoom_range=0.2,\n",
    "                             shear_range=0.1,\n",
    "                             rotation_range=30.)\n",
    "\n",
    "datagen.fit(X)\n",
    "\n",
    "# Reinitialize model and compile\n",
    "model = cnn_model()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Train again\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "steps_per_epoch = int(np.ceil(X_train.shape[0] / float(batch_size)))\n",
    "\n",
    "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size), validation_data=(X_val, Y_val), steps_per_epoch=steps_per_epoch, epochs=epochs, callbacks=[tensorboard], workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_val, Y_val, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit3 = pd.DataFrame(list(zip(test_img_names, y_pred)),columns=['image','class'])\n",
    "df_submit3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export dataframe to csv file for submission\n",
    "df_submit3.to_csv('submission_augmentation.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data preparation\n",
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "                             featurewise_std_normalization=False,\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             zoom_range=0.2,\n",
    "                             shear_range=0.1,\n",
    "                             rotation_range=10.)\n",
    "# fit parameters from data\n",
    "datagen.fit(X)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, Y_batch in datagen.flow(X, Y, batch_size=9, shuffle=False):\n",
    "    # create a grid of 3x3 images\n",
    "    for i in range(0, 9):\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        plt.imshow(X_batch[i].reshape(40, 40), cmap=plt.get_cmap('gray'))\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "\n",
    "# build the VGG16 network\n",
    "#model = applications.VGG16(include_top=False,\n",
    "#                           weights='imagenet')\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "layer_name = 'block5_conv3'\n",
    "filter_index = 0  # can be any integer from 0 to 511, as there are 512 filters in that layer\n",
    "\n",
    "# build a loss function that maximizes the activation\n",
    "# of the nth filter of the layer considered\n",
    "layer_output = layer_dict[layer_name].output\n",
    "loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "# compute the gradient of the input picture wrt this loss\n",
    "grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "# normalization trick: we normalize the gradient\n",
    "grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "\n",
    "# this function returns the loss and grads given the input picture\n",
    "iterate = K.function([input_img], [loss, grads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# we start from a gray image with some noise\n",
    "input_img_data = np.random.random((1, 3, img_width, img_height)) * 20 + 128.\n",
    "# run gradient ascent for 20 steps\n",
    "for i in range(20):\n",
    "    loss_value, grads_value = iterate([input_img_data])\n",
    "    input_img_data += grads_value * step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imsave\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "img = input_img_data[0]\n",
    "img = deprocess_image(img)\n",
    "imsave('%s_filter_%d.png' % (layer_name, filter_index), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
